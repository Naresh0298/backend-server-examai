{
  "message": "File uploaded and OCR processed successfully",
  "original_filename": "test doc.pdf",
  "gcs_info": {
    "gcs_filename": "20250613_003244_83848b8c_test doc.pdf",
    "bucket": "exam_ai_gcs_bucket",
    "size": 149732,
    "content_type": "application/pdf"
  },
  "ocr_results": {
    "full_text": "What is machine\nlearning?\nMachine learning (ML) is a branch of artificial\nintelligence (AI) focused on enabling computers and\nmachines to imitate the way that humans learn, to\nperform tasks autonomously, and to improve their\nperformance and accuracy through experience and\nexposure to more data.\nUC Berkeley breaks out the learning system of a machine learning\nalgorithm into three main parts.\nA Decision Process: In general, machine learning\nalgorithms are used to make a prediction or classification.\nBased on some input data, which can be labeled or\nunlabeled, your algorithm will produce an estimate about a\npattern in the data.\nAn Error Function: An error function evaluates the\nprediction of the model. If there are known examples, an error\nfunction can make a comparison to assess the accuracy of\nthe model.\nA Model Optimization Process: If the model can fit better\nto the data points in the training set, then weights are\nadjusted to reduce the discrepancy between the known\nexample and the model estimate. The algorithm will repeat\nthis iterative \"evaluate and optimize\" process, updating\nweights autonomously until a threshold of accuracy has been\nmet.\nIndustry newsletter\nThe latest Al trends, brought to\nyou by experts\nGet curated insights on the most important—and intriguing-Al\nnews. Subscribe to our weekly Think newsletter. See the IBM\nPrivacy Statement.\nBusiness email\nSubscribe\nMachine learning\nversus deep\nlearning versus\nneural networks\nSince deep learning and machine learning tend to be used\ninterchangeably, it's worth noting the nuances between the two.\nphoto tagging on social media, radiology imaging in healthcare,\nand self-driving cars in the automotive industry.\nRecommendation engines: Using past consumption behavior data,\nAl algorithms can help to discover data trends that can be used to\ndevelop more effective cross-selling strategies. Recommendation\nengines are used by online retailers to make relevant product\nrecommendations to customers during the checkout process.\nRobotic process automation (RPA): Also known as software\nrobotics, RPA uses intelligent automation technologies to perform\nrepetitive manual tasks.\nAutomated stock trading: Designed to optimize stock portfolios,\nAl-driven high-frequency trading platforms make thousands or\neven millions of trades per day without human intervention.\nFraud detection: Banks and other financial institutions can use\nmachine learning to spot suspicious transactions. Supervised\nlearning can train a model using information about known\nfraudulent transactions. Anomaly detection can identify\ntransactions that look atypical and deserve further investigation.\nChallenges of\nmachine learning\nAs machine learning technology has developed, it has certainly\nmade our lives easier. However, implementing machine learning in\nbusinesses has also raised a number of ethical concerns about Al\ntechnologies. Some of these include:\nTechnological singularity\nWhile this topic garners a lot of public attention, many researchers\nare not concerned with the idea of Al surpassing human\nintelligence in the near future. Technological singularity is also\nreferred to as strong Al or superintelligence. Philosopher Nick\nBostrum defines superintelligence as \"any intellect that vastly\noutperforms the best human brains in practically every field,\nincluding scientific creativity, general wisdom, and social skills.\"\nDespite the fact that superintelligence is not imminent in society,\nthe idea of it raises some interesting questions as we consider the\nuse of autonomous systems, like self-driving cars. It's unrealistic to\nthink that a driverless car would never have an accident, but who is\nresponsible and liable under those circumstances? Should we still\ndevelop autonomous vehicles, or do we limit this technology to\nsemi-autonomous vehicles which help people drive safely? The\njury is still out on this, but these are the types of ethical debates\nthat are occurring as new, innovative Al technology develops.\nAl impact on jobs\nWhile a lot of public perception of artificial intelligence centers\naround job losses, this concern should probably be reframed. With\nevery disruptive, new technology, we see that the market demand\nfor specific job roles shifts. For example, when we look at the\nautomotive industry, many manufacturers, like GM, are shifting to\nfocus on electric vehicle production to align with green initiatives.\nThe energy industry isn't going away, but the source of energy is\nshifting from a fuel economy to an electric one.\nIn a similar way, artificial intelligence will shift the demand for jobs\nto other areas. There will need to be individuals to help manage Al\nsystems. There will still need to be people to address more\ncomplex problems within the industries that are most likely to be\naffected by job demand shifts, such as customer service. The\nbiggest challenge with artificial intelligence and its effect on the job\nmarket will be helping people to transition to new roles that are in\ndemand.\nPrivacy\nPrivacy tends to be discussed in the context of data privacy, data\nprotection, and data security. These concerns have allowed\npolicymakers to make more strides in recent years. For example, in\n2016, GDPR legislation was created to protect the personal data of\npeople in the European Union and European Economic Area, giving\nindividuals more control of their data. In the United States,\nindividual states are developing policies, such as the California\nConsumer Privacy Act (CCPA), which was introduced in 2018 and\nrequires businesses to inform consumers about the collection of\ntheir data. Legislation such as this has forced companies to rethink\nhow they store and use personally identifiable information (PII). As\na result, investments in security have become an increasing priority\nfor businesses as they seek to eliminate any vulnerabilities and\nopportunities for surveillance, hacking, and cyberattacks.\nBias and discrimination\nInstances of bias and discrimination across a number of machine\nlearning systems have raised many ethical questions regarding the\nuse of artificial intelligence. How can we safeguard against bias\nand discrimination when the training data itself may be generated\nby biased human processes? While companies typically have good\nintentions for their automation efforts, Reuters: highlights some of\nthe unforeseen consequences of incorporating Al into hiring\npractices. In their effort to automate and simplify a process,\nAmazon unintentionally discriminated against job candidates by\ngender for technical roles, and the company ultimately had to\nscrap the project. Harvard Business Review³ has raised other\npointed questions about the use of Al in hiring practices, such as\nwhat data you should be able to use when evaluating a candidate\nfor a role.\nBias and discrimination aren't limited to the human resources\nfunction either; they can be found in a number of applications from\nfacial recognition software to social media algorithms. As\nbusinesses become more aware of the risks with Al, they've also\nbecome more active in this discussion around Al ethics and values.\nAccountability\nSince there isn't significant legislation to regulate Al practices,\nthere is no real enforcement mechanism to ensure that ethical Al is\npracticed. The current incentives for companies to be ethical are\nthe negative repercussions of an unethical Al system on the bottom\nline. To fill the gap, ethical frameworks have emerged as part of a\ncollaboration between ethicists and researchers to govern the\nconstruction and distribution of Al models within society. However,\nat the moment, these only serve to guide. Some research shows\nthat the combination of distributed responsibility and a lack of\nforesight into potential consequences aren't conducive to\npreventing harm to society.\nHow to choose the\nright Al platform for\nmachine learning\nSelecting a platform can be a challenging process, as the wrong\nsystem can drive up costs, or limit the use of other valuable tools.\nor technologies. When reviewing multiple vendors to select an Al\nplatform, there is often a tendency to think that more features = a\nbetter system. Maybe so, but reviewers should start by thinking\nthrough what the Al platform will be doing for their organization.\nWhat machine learning capabilities need to be delivered and what\nfeatures are important to accomplish them? One missing feature\nmight doom the usefulness of an entire system. Here are some\nfeatures to consider.\nMLOps capabilities. Does the system have:\na unified interface for ease of management?\nautomated machine learning tools for faster model\ncreation with low-code and no-code functionality?\ndecision optimization to streamline the selection and\ndeployment of optimization models?\nvisual modeling to combine visual data science with\nopen-source libraries and notebook-based interfaces on a\nunified data and Al studio?\nautomated development for beginners to get started\nquickly and more advanced data scientists to experiment?\nsynthetic data generator as an alternative or supplement\nto real-world data when real-world data is not readily\navailable?\nGenerative Al capabilities. Does the system have:\na content generator that can generate text, images and\nother content based on the data it was trained on?\nautomated classification to read and classify written\ninput, such as evaluating and sorting customer complaints or\nreviewing customer feedback sentiment?\na summary generator that can transform dense text into\na high-quality summary, capture key points from financial\nreports, and generate meeting transcriptions?\na data extraction capability to sort through complex\ndetails and quickly pull the necessary information from large\ndocuments?\nMachine learning, deep learning, and neural networks are all sub-\nfields of artificial intelligence. However, neural networks is actually\na sub-field of machine learning, and deep learning is a sub-field of\nneural networks.\nThe way in which deep learning and machine learning differ is in\nhow each algorithm learns. \"Deep\" machine learning can use\nlabeled datasets, also known as supervised learning, to inform its\nalgorithm, but it doesn't necessarily require a labeled dataset. The\ndeep learning process can ingest unstructured data in its raw form\n(e.g., text or images), and it can automatically determine the set of\nfeatures which distinguish different categories of data from one\nanother. This eliminates some of the human intervention required\nand enables the use of large amounts of data. You can think of\ndeep learning as \"scalable machine learning\" as Lex Fridman notes\nin this MIT lecture.\nClassical, or \"non-deep,\" machine learning is more dependent on\nhuman intervention to learn. Human experts determine the set of\nfeatures to understand the differences between data inputs, usually\nrequiring more structured data to learn.\nNeural networks, or artificial neural networks (ANNs), are\ncomprised of node layers, containing an input layer, one or more\nhidden layers, and an output layer. Each node, or artificial neuron,\nconnects to another and has an associated weight and threshold. If\nthe output of any individual node is above the specified threshold\nvalue, that node is activated, sending data to the next layer of the\nnetwork. Otherwise, no data is passed along to the next layer of\nthe network by that node. The “deep” in deep learning is just\nreferring to the number of layers in a neural network. A neural\nnetwork that consists of more than three layers, which would be\ninclusive of the input and the output can be considered a deep\nlearning algorithm or a deep neural network. A neural network that\nonly has three layers is just a basic neural network.\nDeep learning and neural networks are credited with accelerating\nprogress in areas such as computer vision, natural language\nprocessing (NLP), and speech recognition.\nSee the blog post \"Al vs. Machine Learning vs. Deep Learning vs.\nNeural Networks: What's the Difference?\" for a closer look at how\nthe different concepts relate.\nMixture of Experts | 6 June, episode 58\nEp.58\nNY Tech Week,\nopen source AI reports\nand Claude 4 behaviors\n0\nIBM\n900\n94\nO\nO\nD\nO\n0\nO\n0\n9\n0\n0\nDecoding Al: Weekly News Roundup\nJoin our world-class panel of engineers, researchers, product\nleaders and more as they cut through the Al noise to bring you the\nlatest in Al news and insights.\nWatch the latest podcast episodes\nMachine\nlearning methods\nMachine learning models fall into three primary categories.\nSupervised learning\nSupervised learning, also known as supervised machine learning,\nis defined by its use of labeled datasets to train algorithms to\nclassify data or predict outcomes accurately. As input data is fed\ninto the model, the model adjusts its weights until it has been fitted\nappropriately. This occurs as part of the cross validation process to\nensure that the model avoids overfitting or underfitting. Supervised\nlearning helps organizations solve a variety of real-world problems\nat scale, such as classifying spam in a separate folder from your\ninbox. Some methods used in supervised learning include neural\nnetworks, Naïve Bayes, linear regression, logistic regression,\nrandom forest, and support vector machine (SVM).\nUnsupervised learning\nUnsupervised learning, also known as unsupervised machine\nlearning, uses machine learning algorithms to analyze and cluster\nunlabeled datasets (subsets called clusters). These algorithms\ndiscover hidden patterns or data groupings without the need for\nhuman intervention.\nUnsupervised learning's ability to discover similarities and\ndifferences in information make it ideal for exploratory data\nanalysis, cross-selling strategies, customer segmentation, and\nimage and pattern recognition. It's also used to reduce the number\nof features in a model through the process of dimensionality\nreduction. Principal component analysis (PCA) and singular value\ndecomposition (SVD) are two common approaches for this. Other\nalgorithms used in unsupervised learning include neural networks,\nk-means clustering, and probabilistic clustering methods.\nSemi-supervised learning\nSemi-supervised learning offers a happy medium between\nsupervised and unsupervised learning. During training, it uses a\nsmaller labeled data set to guide classification and feature\nextraction from a larger, unlabeled data set. Semi-supervised\nlearning can solve the problem of not having enough labeled data\nfor a supervised learning algorithm. It also helps if it's too costly to\nlabel enough data.\nFor a deep dive into the differences between these approaches,\ncheck out \"Supervised vs. Unsupervised Learning: What's the\nDifference?\"\nReinforcement learning\nReinforcement learning is a machine learning model that is similar\nto supervised learning, but the algorithm isn't trained using sample\ndata. This model learns as it goes by using trial and error. A\nsequence of successful outcomes will be reinforced to develop the\nbest recommendation or policy for a given problem.\nThe IBM WatsonⓇ system that won the Jeopardy! challenge in\n2011 is a good example. The system used reinforcement\nlearning to learn when to attempt an answer (or question, as it\nwere), which square to select on the board, and how much to\nwager, especially on daily doubles.\nCommon machine\nlearning algorithms\nA number of machine learning algorithms are commonly used.\nThese include:\nNeural networks\nLinear regression\nLogistic regression\nClustering\nDecision trees\nRandom forests\nNeural networks\nNeural networks simulate the way the human brain works, with a\nhuge number of linked processing nodes. Neural networks are\ngood at recognizing patterns and play an important role in\napplications including natural language translation, image\nrecognition, speech recognition, and image creation.\nLinear regression\nThis algorithm is used to predict numerical values, based on\na linear relationship between different values. For example, the\ntechnique could be used to predict house prices based on\nhistorical data for the area.\nLogistic regression\nThis supervised learning algorithm makes predictions for\ncategorical response variables, such as \"yes/no\" answers to\nquestions. It can be used for applications such as classifying spam\nand quality control on a production line.\nClustering\nUsing unsupervised learning, clustering algorithms can identify\npatterns in data so that it can be grouped. Computers can help\ndata scientists by identifying differences between data items that\nhumans have overlooked.\nDecision trees\nDecision trees can be used for both predicting numerical values\n(regression) and classifying data into categories. Decision trees use\na branching sequence of linked decisions that can be represented\nwith a tree diagram. One of the advantages of decision trees is that\nthey are easy to validate and audit, unlike the black box of the\nneural network.\nRandom forests\nIn a random forest, the machine learning algorithm predicts a value\nor category by combining the results from a number of decision\ntrees.\nAdvantages and\ndisadvantages of\nmachine learning\nalgorithms\nDepending on your budget, need for speed and precision required,\neach algorithm type supervised, unsupervised, semi-supervised, or\nreinforcement has its own advantages and disadvantages.\nFor example, decision tree algorithms are used for both predicting\nnumerical values (regression problems) and classifying data into\ncategories. Decision trees use a branching sequence of linked\ndecisions that may be represented with a tree diagram. A prime\nadvantage of decision trees is that they are easier to validate and\naudit than a neural network. The bad news is that they can be more\nunstable than other decision predictors.\nOverall, there are many advantages to machine learning that\nbusinesses can leverage for new efficiencies. These include\nmachine learning identifying patterns and trends in massive\nvolumes of data that humans might not spot at all. And this\nanalysis requires little human intervention: just feed in the dataset\nof interest and let the machine learning system assemble and refine\nits own algorithms, which will continually improve with more data\ninput over time. Customers and users can enjoy a more\npersonalized experience as the model learns more with every\nexperience with that person.\nOn the downside, machine learning requires large training datasets\nthat are accurate and unbiased. GIGO is the operative factor:\ngarbage in garbage out. Gathering sufficient data and having a\nsystem robust enough to run it might also be a drain on resources.\nMachine learning can also be prone to error, depending on the\ninput. With too small a sample, the system could produce a\nperfectly logical algorithm that is completely wrong or misleading.\nTo avoid wasting budget or displeasing customers, organizations\nshould act on the answers only when there is high confidence in\nthe output.\nReal-world\nmachine learning\nuse cases\nHere are just a few examples of machine learning you might\nencounter every day:\nGenerative Al: Generative Al, or gen Al, is machine learning that\ncan create original content-text, images, video, software code-in\nresponse to a user's prompt or request. Gen Al relies on deep\nlearning models that identify and encode the patterns and\nrelationships in huge amounts of data, and then use that\ninformation to understand users' requests and create new content.\nChatGPT and Claude.ai are examples of generative Al apps.\nAl agents and agentic Al: An Al agent is an autonomous Al\nprogram-it can perform tasks and accomplish goals on behalf of a\nuser or another system without human intervention, by designing\nits own workflow and using available tools (other applications or\nservices). Agentic Al is a system of multiple Al agents, the efforts of\nwhich are coordinated, or orchestrated, to accomplish a more\ncomplex task or a greater goal than any single agent in the system\ncould accomplish.\nExplore our 2025 guide to Al agents\nSpeech recognition: Also known as automatic speech recognition\n(ASR), computer speech recognition, or speech-to-text, speech\nrecognition uses natural language processing (NLP) to translate\nhuman speech into a written format. Many mobile devices\nincorporate speech recognition into their systems to conduct voice.\nsearch e.g. Siri or improve accessibility for texting.\nCustomer service: Online chatbots are replacing human agents\nalong the customer journey, changing the way we think about\ncustomer engagement across websites and social media\nplatforms. Chatbots answer frequently asked questions (FAQs)\nabout topics such as shipping, or provide personalized advice,\ncross-selling products or suggesting sizes for users. Examples\ninclude virtual agents on e-commerce sites; messaging bots, using\nSlack and Facebook Messenger; and tasks usually done by virtual\nassistants and voice assistants.\nComputer vision: This Al technology enables computers to derive\nmeaningful information from digital images, videos, and other\nvisual inputs, and then take the appropriate action. Powered by\nconvolutional neural networks, computer vision has applications in",
    "structured_data": [],
  }
}